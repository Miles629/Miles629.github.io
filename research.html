<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width,initial-scale=1,shrink-to-fit=no">
    <title>Research - Caoyuan Ma</title>
    <link href="assets/css/pascalmichaillat.css" rel="stylesheet">
</head>
<body class="list" id="top">
    <header class="header">
        <nav class="nav">
            <div class="logo">
                <a href="index.html">Caoyuan Ma</a>
            </div>
            <ul id="menu">
                <li><a href="research.html"><span class="active">Research</span></a></li>
                <li><a href="news.html"><span>News</span></a></li>
                <li><a href="misc.html"><span>Misc</span></a></li>
            </ul>
        </nav>
    </header>
    <main class="main">
        <header class="page-header">
            <h1>Research</h1>
        </header>
        <h2>Research Interests</h2>
        <ul>
            <li>Human-centric AIGC</li>
            <li>Generative Computational Photography</li>
            <li>Embodied AI</li>
            <li>Multimodal Large Language Models</li>
            <li>AGI</li>
        </ul>

        <h2>Publications</h2>

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">SafeWork-R1: Coevolving Safety and Intelligence under the AI-45 Law</h2>
            </header>
            <div class="entry-content">
                <p>SAI Labs, Y Bao, etc. <strong>Caoyuan Ma</strong><br>
                <em>arXiv</em>, 2025</p>
            </div>
            <footer class="entry-footer">
                <a href="">Paper</a>
            </footer>
        </article>


        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">TAAT: Think and Act from Arbitrary Texts</h2>
            </header>
            <div class="entry-content">
                <p>Runqi Wang*, <strong>Caoyuan Ma*</strong>, Guopeng Li*, Zheng Wang<br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2025</p>
            </div>
            <footer class="entry-footer">
                <a href="https://arxiv.org/abs/2404.14745">Paper</a>
            </footer>
        </article>

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion</h2>
            </header>
            <div class="entry-content">
                <p>Haoyang Chen*, Dongfang Sun*, <strong>Caoyuan Ma*</strong>, Shiqin Wang, Kewei Zhang, Zheng Wang, Zhixiang Wang<br>
                <em>International Conference on Computer Vision (ICCV)</em>, 2025</p>
            </div>
            <footer class="entry-footer">
                <a href="https://arxiv.org/pdf/2506.23711">Paper</a>
            </footer>
        </article>

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">Leader and Follower: Interactive Motion Generation under Trajectory Constraints</h2>
            </header>
            <div class="entry-content">
                <p>Runqi Wang*, <strong>Caoyuan Ma*</strong>, Jian Zhao, Hanrui Xu, Dongfang Sun, Haoyang Chen, Lin Xiong, Zheng Wang, Xuelong Li<br>
                <em>ACM International Conference on Multimedia (ACM MM)</em>, 2024</p>
            </div>
            <footer class="entry-footer">
                <a href="https://arxiv.org/pdf/2502.11563">Paper</a>
            </footer>
        </article>

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer</h2>
            </header>
            <div class="entry-content">
                <p>Zhimin Zhang*, Bi'an Du*, <strong>Caoyuan Ma</strong>, Zheng Wang, Wei Hu<br>
                <em>ACM International Conference on Multimedia (ACM MM)</em>, 2025</p>
            </div>
            <footer class="entry-footer">
                <a href="">Paper</a>
            </footer>
        </article>

        

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">Causal deciphering and inpainting in spatio-temporal dynamics via diffusion model</h2>
            </header>
            <div class="entry-content">
                <p>Yifan Duan, Jian Zhao, Junyuan Mao, Hao Wu, Jingyu Xu, <strong>Caoyuan Ma</strong>, Kai Wang, Kun Wang, Xuelong Li<br>
                <em>Neural Information Processing Systems (NeurIPS)</em>, 2024</p>
            </div>
            <footer class="entry-footer">
                <a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/c2d82a425af4c18a35049899fea5ee82-Paper-Conference.pdf">Paper</a>
            </footer>
        </article>

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">HumanNeRF-SE: A Simple yet Effective Approach to Animate HumanNeRF with Diverse Poses</h2>
            </header>
            <div class="entry-content">
                <p><strong>Caoyuan Ma</strong>, Yu-Lun Liu, Zhixiang Wang, Wu Liu, Xinchen Liu, Zheng Wang<br>
                <em>Computer Vision and Pattern Recognition (CVPR)</em>, 2024</p>
            </div>
            <footer class="entry-footer">
                <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Ma_HumanNeRF-SE_A_Simple_yet_Effective_Approach_to_Animate_HumanNeRF_with_CVPR_2024_paper.pdf">Paper</a>
                &middot;
                <a href="https://miles629.github.io/humannerf-se.github.io/">Project Page</a>
                &middot;
                <a href="https://github.com/Miles629/HumanNeRF-SE">Code</a>
            </footer>
        </article>

        

        <article class="post-entry">
            <header class="entry-header">
                <h2 class="entry-hint-parent">Improving Stack Overflow question title generation with copying enhanced CodeBERT model and bi-modal information</h2>
            </header>
            <div class="entry-content">
                <p>Fengji Zhang, Xiao Yu, Jacky Keung, Fuyang Li, Zhiwen Xie, Zhen Yang, <strong>Caoyuan Ma</strong>, Zhimin Zhang<br>
                <em>Information and Software Technology</em>, 2022</p>
            </div>
            <footer class="entry-footer">
                <a href="https://www.sciencedirect.com/science/article/pii/S0950584922000763">Paper</a>
                &middot;
                <a href="https://github.com/zfj1998/Copying-enhanced-CodeBERT">Code</a>
            </footer>
        </article>
    </main>
</body>
</html>
